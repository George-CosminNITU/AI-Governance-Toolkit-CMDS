# Project Proposal: Corporate AI Governance & Acceptable Use Framework
**Student:** George-Cosmin NITU  
**Program:** CMDS 2025-2027  

## 1. Project Summary
This project aims to develop a comprehensive **Corporate AI Governance Framework** designed to help organizations manage the security, legal, and operational risks associated with the adoption of Generative AI tools (e.g., ChatGPT, Microsoft Copilot, Gemini). 

As employees increasingly adopt "Shadow AI" for productivity, organizations face significant risks regarding data leakage, intellectual property rights, and regulatory non-compliance. The final deliverable will be a ready-to-implement **"AI Compliance Toolkit"** containing acceptable use policies (AUP), data classification guidelines for AI inputs, and a risk assessment methodology aligned with the EU AI Act.

## 2. Needs & Motivation
* **Unregulated Adoption:** "Shadow AI" is rampant; employees are feeding sensitive corporate data (emails, code, customer lists) into public AI models without understanding the privacy implications.
* **Regulatory Pressure:** The **EU AI Act** imposes strict transparency and governance obligations. Companies need a structured way to demonstrate they are in control of their AI usage to avoid heavy fines.
* **Data Sovereignty & Privacy:** Standard contracts with US-based AI providers often conflict with GDPR requirements. Organizations need a clear protocol to evaluate which tools are safe to use.
* **Operational Clarity:** HR and IT departments lack a standardized process to approve or deny requests for new AI tools, leading to inconsistency and security gaps.

## 3. Objectives
1. **Analyze** the security and privacy risks posed by the most common Generative AI tools used in enterprise environments (e.g., data retention policies of OpenAI, Microsoft, Google).
2. **Develop** a tiered **"Acceptable Use Policy" (AUP)** that clearly defines what data employees can and cannot send to AI platforms (e.g., "Public" vs. "Internal" vs. "Confidential").
3. **Create** a **Risk Assessment Matrix** for vetting new AI software, allowing IT managers to quickly score tools based on compliance with GDPR and the EU AI Act.
4. **Design** a "Human-in-the-Loop" workflow to ensure that AI-generated content is reviewed for accuracy, bias, and copyright infringement before being published.

## 4. State of Things / Related Work
* **Current State:** Most organizations currently rely on total bans (which are ignored by staff) or have no policy at all. Security teams often lack visibility into what AI tools are actually running on the network.
* **Existing Standards:** While high-level standards like **ISO/IEC 42001 (AI Management Systems)** exist, they are abstract and difficult for small-to-medium enterprises to implement practically.
* **The Gap:** There is a lack of "operational" frameworks—simple, actionable documents and checklists that an IT Manager can deploy immediately without hiring expensive legal consultants.

## 5. Proposed Solution & Methodology
The project will deliver a practical **"AI Governance Toolkit"** that serves as an operational program for managing AI risk. The toolkit will consist of:
* **The Governance Policy:** A formal document outlining roles, responsibilities, and prohibited use cases for AI.
* **The Data Classification Guide:** A visual reference for employees showing which data types are safe for public AI vs. private enterprise AI.
* **The Vendor Vetting Checklist:** A spreadsheet-based tool for assessing third-party AI tools against security and privacy criteria.
* **Training Material:** A short module for employee awareness regarding AI security risks.

### Methodology Phases
* **Phase 1: Research & Analysis:** Deep dive into the legal requirements (EU AI Act, GDPR) and technical analysis of data retention policies of major AI providers.
* **Phase 2: Framework Design:** Drafting the core operational documents, including the AUP and Data Classification standards, aligned with NIST AI RMF.
* **Phase 3: Tool Development:** Creating the practical risk assessment tools (Excel-based Vendor Vetting Checklist and Risk Matrix).
* **Phase 4: Validation:** Testing the complete toolkit against a real-world or hypothetical corporate scenario to verify its effectiveness and usability.

## 6. Project Roadmap
### YEAR 1, Semester 1 (Oct 2025 – Feb 2026)
* **Focus:** State of the Art & Initial Framework Design.
* **Deliverable:** Git Repository containing the Project Report (State of the Art) and the initial skeleton of the Acceptable Use Policy.

### YEAR 1, Semester 2 (Feb 2026 – June 2026)
* **Focus:** Core Governance Documentation.
* **Deliverable:** First functional version of the Governance Toolkit (Full AUP and Data Classification Guide).

### YEAR 2, Semester 1 (Oct 2026 – Feb 2027)
* **Focus:** Advanced Risk Management Tools.
* **Deliverable:** Vendor Vetting Checklist and Risk Assessment Matrix.

### YEAR 2, Semester 2 (Feb 2027 – June 2027)
* **Focus:** Validation & Final Thesis.
* **Deliverable:** Final Master Thesis and validation results.
